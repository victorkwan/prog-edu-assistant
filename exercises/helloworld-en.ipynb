{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Hello world\n",
    "\n",
    "In this unit you will learn how to use Python to implement the first ever program\n",
    "that *every* programmer starts with. This also serves as an example for the master\n",
    "notebook format.\n",
    "\n",
    "```\n",
    "# All markdown cells are searched for triple-backtick blocks. Within a triple quoted block,\n",
    "# a match on /^# ASSIGNMENT METADATA/ will trigger handling this as an assignment-level metadata\n",
    "# block. The rest of the metadata may be used e.g. for setting default settings for autograder\n",
    "# isolation (memory limit etc). The assignment_id is useful to identify which assignment a submission\n",
    "# pertains to.\n",
    "# ASSIGNMENT METADATA\n",
    "assignment_id = \"HelloWorld\"\n",
    "```\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Here is the traditional first programming exercise, called \"Hello world\".\n",
    "The task is to print the message: \"Hello, world\".\n",
    "\n",
    "Here are a few examples to get you started. Run the following cells and see how\n",
    "you can print a message. To run a cell, click with mouse inside a cell, then\n",
    "press Ctrl+Enter to execute it. If you want to execute a few cells sequentially,\n",
    "then press Shift+Enter instead, and the focus will be automatically moved\n",
    "to the next cell as soon as one cell finishes execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bye bye\n"
     ]
    }
   ],
   "source": [
    "print(\"bye bye\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hey you\n"
     ]
    }
   ],
   "source": [
    "print(\"hey\", \"you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one\n",
      "two\n"
     ]
    }
   ],
   "source": [
    "print(\"one\")\n",
    "print(\"two\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import contextmanager\n",
    "from io import StringIO\n",
    "\n",
    "@contextmanager\n",
    "def capture_output():\n",
    "    capture_out, capture_err = StringIO(), StringIO()\n",
    "    save_out, save_err = sys.stdout, sys.stderr\n",
    "    try:\n",
    "        sys.stdout, sys.stderr = capture_out, capture_err\n",
    "        yield sys.stdout, sys.stderr\n",
    "    finally:\n",
    "        sys.stdout, sys.stderr = save_out, save_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO(salikh): Move this into a shared library and make that library installable via pip.\n",
    "class SummaryTestResult(unittest.TextTestResult):\n",
    "    \"\"\"A small extension of TextTestResult that also collects a map of test statuses.\n",
    "    \n",
    "    \n",
    "    result.results is a map from test name (string) to boolean: True(passed) or False(failed or error)\"\"\"\n",
    "    \n",
    "    separator1 = '=' * 70\n",
    "    separator2 = '-' * 70\n",
    "    \n",
    "    def __init__(self, stream, descriptions, verbosity):\n",
    "        super(unittest.TextTestResult, self).__init__(stream, descriptions, verbosity)\n",
    "        # A map of test name to True(passed) or False(failed or error)\n",
    "        self.results = {}\n",
    "        # Copied from TextTestResult.\n",
    "        self.stream = stream\n",
    "        self.showAll = verbosity > 1\n",
    "        self.dots = verbosity == 1\n",
    "        self.descriptions = descriptions\n",
    "\n",
    "    def testName(self, test):\n",
    "        \"\"\"A helper function to format the test as a human-readable string.\n",
    "        \n",
    "        The format is TestClassName.test_method. This is similar\n",
    "        to TextTestResult.getDescription(test), but uses different format.\n",
    "        getDescription: 'test_one (__main__.HelloTest)'\n",
    "        testName: 'HelloTest.test_one'\n",
    "        \"\"\"\n",
    "        return unittest.util.strclass(test.__class__).replace('__main__.', '') + '.' + test._testMethodName\n",
    "        \n",
    "    def getDescription(self, test):\n",
    "        doc_first_line = test.shortDescription()\n",
    "        if self.descriptions and doc_first_line:\n",
    "            return '\\n'.join((str(test), doc_first_line))\n",
    "        else:\n",
    "            return str(test)\n",
    "\n",
    "    def startTest(self, test):\n",
    "        super(unittest.TextTestResult, self).startTest(test)\n",
    "        if self.showAll:\n",
    "            self.stream.write(self.getDescription(test))\n",
    "            self.stream.write(\" ... \")\n",
    "            self.stream.flush()\n",
    "\n",
    "    def addSuccess(self, test):\n",
    "        super(unittest.TextTestResult, self).addSuccess(test)\n",
    "        if self.showAll:\n",
    "            self.stream.writeln(\"ok\")\n",
    "        elif self.dots:\n",
    "            self.stream.write('.')\n",
    "            self.stream.flush()\n",
    "        self.results[self.testName(test)] = True\n",
    "\n",
    "    def addError(self, test, err):\n",
    "        super(unittest.TextTestResult, self).addError(test, err)\n",
    "        if self.showAll:\n",
    "            self.stream.writeln(\"ERROR\")\n",
    "        elif self.dots:\n",
    "            self.stream.write('E')\n",
    "            self.stream.flush()\n",
    "        self.results[self.testName(test)] = False\n",
    "\n",
    "    def addFailure(self, test, err):\n",
    "        super(unittest.TextTestResult, self).addFailure(test, err)\n",
    "        if self.showAll:\n",
    "            self.stream.writeln(\"FAIL\")\n",
    "        elif self.dots:\n",
    "            self.stream.write('F')\n",
    "            self.stream.flush()\n",
    "        self.results[self.testName(test)] = False\n",
    "\n",
    "    def addSkip(self, test, reason):\n",
    "        super(unittest.TextTestResult, self).addSkip(test, reason)\n",
    "        if self.showAll:\n",
    "            self.stream.writeln(\"skipped {0!r}\".format(reason))\n",
    "        elif self.dots:\n",
    "            self.stream.write(\"s\")\n",
    "            self.stream.flush()\n",
    "\n",
    "    def addExpectedFailure(self, test, err):\n",
    "        super(unittest.TextTestResult, self).addExpectedFailure(test, err)\n",
    "        if self.showAll:\n",
    "            self.stream.writeln(\"expected failure\")\n",
    "        elif self.dots:\n",
    "            self.stream.write(\"x\")\n",
    "            self.stream.flush()\n",
    "\n",
    "    def addUnexpectedSuccess(self, test):\n",
    "        super(unittest.TextTestResult, self).addUnexpectedSuccess(test)\n",
    "        if self.showAll:\n",
    "            self.stream.writeln(\"unexpected success\")\n",
    "        elif self.dots:\n",
    "            self.stream.write(\"u\")\n",
    "            self.stream.flush()\n",
    "\n",
    "    def printErrors(self):\n",
    "        if self.dots or self.showAll:\n",
    "            self.stream.writeln()\n",
    "        self.printErrorList('ERROR', self.errors)\n",
    "        self.printErrorList('FAIL', self.failures)\n",
    "\n",
    "    def printErrorList(self, flavour, errors):\n",
    "        for test, err in errors:\n",
    "            self.stream.writeln(self.separator1)\n",
    "            self.stream.writeln(\"%s: %s\" % (flavour,self.getDescription(test)))\n",
    "            self.stream.writeln(self.separator2)\n",
    "            self.stream.writeln(\"%s\" % err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "```\n",
    "# The markdown cell with triple-backtick block matching /^# EXERCISE METADATA/ is an exercise-level\n",
    "# metadata. The next block is assumed to be the solution block, and will get annotated with\n",
    "# the exercise_id.\n",
    "# EXERCISE METADATA\n",
    "exercise_id = \"hello1\"\n",
    "```\n",
    "\n",
    "Now it is your turn. Please create a program in the next cell that would print a message \"Hello, world\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, world\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'# BEGIN PROMPT\\n# ... put your program here\\n# END PROMPT'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BEGIN SOLUTION\n",
    "print(\"Hello, world\")\n",
    "# END SOLUTION\n",
    "\"\"\"# BEGIN PROMPT\n",
    "# ... put your program here\n",
    "# END PROMPT\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above cell, the lines marked with `# SOLUTION` or between `# BEGIN SOLUTION` and `# END SOLUTION`\n",
    "will be removed (either by a some dumb heuristic based-replacement rule or by a provided prompt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST OUTPUT\n",
    "# The cells marked with \"^# TEST OUTPUT\" will be run and the output would be checked against\n",
    "# the output in the master notebook.\n",
    "hello()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise for autograding output\n",
    "\n",
    "```\n",
    "# EXERCISE METADATA\n",
    "exercise_id=\"hello2\"\n",
    "```\n",
    "\n",
    "Please create a function that given a name string returns a string with a greeting,\n",
    "For example, for input `\"world\"` it should return `\"Hello, world\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hello(name):\n",
    "    \"\"\"BEGIN PROMPT\n",
    "    # Please put your solution here:\n",
    "    # return ...\n",
    "    pass\n",
    "    \"\"\" # END PROMPT\n",
    "    # BEGIN SOLUTION\n",
    "    return \"Hello, \" + name\n",
    "    # END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "assert(hello(\"world\") == \"Hello, world\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A code cell marked with `\"# TEST\"` will be converted into a unit test for the solution. It will also be preserved in the student version of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_output (__main__.HelloOutputTest) ... ERROR\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_output (__main__.HelloOutputTest)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-148-7902729379b1>\", line 10, in test_output\n",
      "    submission.hello()\n",
      "TypeError: hello() missing 1 required positional argument: 'name'\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.001s\n",
      "\n",
      "FAILED (errors=1)\n",
      "\n",
      "{'HelloOutputTest.test_output': False}\n"
     ]
    }
   ],
   "source": [
    "class Submission(object): pass\n",
    "submission = Submission()\n",
    "submission.printHello = printHello\n",
    "# BEGIN UNITTEST\n",
    "import unittest\n",
    "\n",
    "class HelloOutputTest(unittest.TestCase):\n",
    "    def test_output(self):\n",
    "        with capture_output() as (out, err):\n",
    "            submission.printHello()\n",
    "        self.assertEquals(err.get_value(), \"\")\n",
    "        self.assertEquals(out.get_value(), \"Hello, world\")\n",
    "\n",
    "# END UNITTEST\n",
    "\n",
    "import sys\n",
    "import io\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(HelloOutputTest)\n",
    "errors = io.StringIO()\n",
    "result = unittest.TextTestRunner(verbosity=4,stream=errors, resultclass=SummaryTestResult).run(suite)\n",
    "# Optional.\n",
    "print(errors.getvalue())\n",
    "\n",
    "print(result.results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_bad (__main__.HelloTest) ... FAIL\n",
      "test_one (__main__.HelloTest) ... ok\n",
      "\n",
      "======================================================================\n",
      "FAIL: test_bad (__main__.HelloTest)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-135-1105af7a4dff>\", line 20, in test_bad\n",
      "    self.assertEqual(submission.hello(\"bad\"), \"Hello, good\")\n",
      "AssertionError: 'Hello, bad' != 'Hello, good'\n",
      "- Hello, bad\n",
      "?        ^^\n",
      "+ Hello, good\n",
      "?        ^^^\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.001s\n",
      "\n",
      "FAILED (failures=1)\n",
      "\n",
      "{'HelloTest.test_one': True, 'HelloTest.test_bad': False}\n"
     ]
    }
   ],
   "source": [
    "# The part before \"BEGIN UNITTEST\" -- preamble -- sets up the environment so that 'submission.hello'\n",
    "# is a function that we need to test. In the autograder worker environment, the preamble will be\n",
    "# replaced with 'import submission' with an assumption that the student's solution will be written\n",
    "# to the file 'submission.py'.\n",
    "class Submission(object): pass\n",
    "submission = Submission()\n",
    "submission.hello = hello\n",
    "\n",
    "# BEGIN UNITTEST\n",
    "# The unit tests main part is contained between \"BEGIN UNITTEST\" and \"END UNITTEST\". It will be copied\n",
    "# verbatim into the autograder directory with an appro\n",
    "\n",
    "import unittest\n",
    "\n",
    "class HelloTest(unittest.TestCase):\n",
    "    def test_one(self):\n",
    "        self.assertEqual(submission.hello(\"one\"), \"Hello, one\")\n",
    "        \n",
    "    def test_bad(self):\n",
    "        self.assertEqual(submission.hello(\"bad\"), \"Hello, good\")\n",
    "        \n",
    "# END UNITTEST\n",
    "\n",
    "import sys\n",
    "import io\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(HelloTest)\n",
    "errors = io.StringIO()\n",
    "result = unittest.TextTestRunner(verbosity=4,stream=errors, resultclass=SummaryTestResult).run(suite)\n",
    "# Optional.\n",
    "print(errors.getvalue())\n",
    "\n",
    "print(result.results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(<__main__.HelloTest testMethod=test_bad>,\n",
       "  'Traceback (most recent call last):\\n  File \"<ipython-input-135-1105af7a4dff>\", line 20, in test_bad\\n    self.assertEqual(submission.hello(\"bad\"), \"Hello, good\")\\nAssertionError: \\'Hello, bad\\' != \\'Hello, good\\'\\n- Hello, bad\\n?        ^^\\n+ Hello, good\\n?        ^^^\\n\\n')]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_bad (__main__.HelloTest) ... FAIL\n",
      "test_one (__main__.HelloTest) ... FAIL\n",
      "\n",
      "======================================================================\n",
      "FAIL: test_bad (__main__.HelloTest)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-135-1105af7a4dff>\", line 20, in test_bad\n",
      "    self.assertEqual(submission.hello(\"bad\"), \"Hello, good\")\n",
      "AssertionError: 'Bye, bad' != 'Hello, good'\n",
      "- Bye, bad\n",
      "+ Hello, good\n",
      "\n",
      "\n",
      "======================================================================\n",
      "FAIL: test_one (__main__.HelloTest)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-135-1105af7a4dff>\", line 17, in test_one\n",
      "    self.assertEqual(submission.hello(\"one\"), \"Hello, one\")\n",
      "AssertionError: 'Bye, one' != 'Hello, one'\n",
      "- Bye, one\n",
      "+ Hello, one\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.001s\n",
      "\n",
      "FAILED (failures=2)\n",
      "\n",
      "{'HelloTest.test_one': False, 'HelloTest.test_bad': False}\n"
     ]
    }
   ],
   "source": [
    "# BEGIN AUTOGRADER TEST\n",
    "# The tests below test that the unit test suite above produces expected outcomes from\n",
    "def wrong_hello(name):\n",
    "    return \"Bye, \" + name\n",
    "\n",
    "submission.hello = wrong_hello\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(HelloTest)\n",
    "errors = io.StringIO()\n",
    "result = unittest.TextTestRunner(verbosity=4,stream=errors,  resultclass=SummaryTestResult).run(suite)\n",
    "# Optional.\n",
    "print(errors.getvalue())\n",
    "print(result.results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test_bad (__main__.HelloTest)'"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.getDescription(result.failures[0][0])"
   ]
  }
 ],
 "metadata": {
  "course_info": {
   "course_name": "cs101",
   "unit_name": "helloworld"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
